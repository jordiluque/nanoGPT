{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVZDLeXrwuMn"
      },
      "source": [
        "# Initialization\n",
        "Shakespeare example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "21DioIIwrGsE"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi7GT1jFpmmh",
        "outputId": "d727c5d4-7973-4709-aa1a-929b4521852f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 96 (delta 29), reused 84 (delta 20), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (96/96), 3.14 MiB | 6.53 MiB/s, done.\n",
            "/content/nanoGPT\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Obtenemos nanoGPT y las librerías necesarias\n",
        "%cd /content/\n",
        "!git clone https://github.com/jordiluque/nanoGPT\n",
        "\n",
        "%cd nanoGPT\n",
        "!pip3 install -r requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXQJAEOCa1Tl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9tlGSy6NPG5",
        "outputId": "f1b6b5e9-3a40-47b0-8ba1-8965dee7918d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import os\n",
            "import requests\n",
            "import tiktoken\n",
            "import numpy as np\n",
            "import logging\n",
            "\n",
            "class Dataset:\n",
            "    def __init__(self, data_url = None) -> None:\n",
            "        # log event\n",
            "        logging.debug(f\"Dataset created with data_url: {data_url}\")\n",
            "        self.data_url = data_url\n",
            "        self.input_data = None\n",
            "        self.train_ids = None\n",
            "        self.val_ids = None\n",
            "\n",
            "    def fetch(self):\n",
            "        data_url = self.data_url or 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt' # shakespeare example if not set\n",
            "        logging.debug(f\"Fetching data from {data_url}\")\n",
            "        self.input_data = requests.get(data_url, timeout=1024).text\n",
            "\n",
            "    def save(self, path):\n",
            "        with open(path, 'w', encoding=\"utf-8\") as f:\n",
            "            f.write(self.input_data)\n",
            "    \n",
            "    def load(self, path):\n",
            "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
            "            self.input_data = f.read()\n",
            "\n",
            "    def parse(self):\n",
            "        data = self.input_data\n",
            "        n = len(data)\n",
            "        train_data = data[:int(n*0.9)]\n",
            "        val_data = data[int(n*0.9):]\n",
            "\n",
            "        logging.debug(f\"Parsing data with {n} characters\")\n",
            "\n",
            "        # encode with tiktoken gpt2 bpe\n",
            "        enc = tiktoken.get_encoding(\"gpt2\")\n",
            "        train_ids = enc.encode_ordinary(train_data)\n",
            "        val_ids = enc.encode_ordinary(val_data)\n",
            "        print(f\"train has {len(train_ids)} tokens\")\n",
            "        print(f\"val has {len(val_ids)} tokens\")\n",
            "\n",
            "        # export to bin files\n",
            "        self.train_ids = np.array(train_ids, dtype=np.uint16)\n",
            "        self.val_ids = np.array(val_ids, dtype=np.uint16)\n",
            "    \n",
            "    def export(self, path):\n",
            "        self.train_ids.tofile(path + 'train.bin')\n",
            "        self.val_ids.tofile(path + 'val.bin')\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    # Set logging to debug\n",
            "    logging.basicConfig(level=logging.DEBUG)\n",
            "    # Example using shakespeare\n",
            "    ds = Dataset()\n",
            "    if not os.path.exists('input.txt'):\n",
            "        ds.fetch()\n",
            "        ds.save('input.txt')\n",
            "    ds.load('input.txt')\n",
            "    ds.parse()\n",
            "    ds.export('./')"
          ]
        }
      ],
      "source": [
        "!cat /content/nanoGPT/data/shakespeare/prepare.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G8LKje9rRb0",
        "outputId": "8e5f631c-b91c-4cc5-eba1-b61f2d845364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n",
            "train has 301966 tokens\n",
            "val has 36059 tokens\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Descargamos y curamos los datos\n",
        "%cd /content/nanoGPT/\n",
        "#!pip install codecarbon --quiet\n",
        "#!codecarbon init\n",
        "#!echo \"log_level = CRITICAL\" >> .codecarbon.config\n",
        "#!echo \"save_to_api = True\" >> .codecarbon.config\n",
        "from data.shakespeare.prepare import Dataset\n",
        "import logging, os\n",
        "\n",
        "# Set logging to debug\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "# Example using shakespeare\n",
        "ds = Dataset()\n",
        "ds.fetch()\n",
        "ds.save('input.txt')\n",
        "ds.load('input.txt')\n",
        "ds.parse()\n",
        "ds.export('./data/shakespeare/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JChEzslSsGsF",
        "outputId": "8e2a5e04-0c68-4b91-8249-c89fde53293b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First Citizen:\n",
            "Soft! who comes here?\n",
            "\n",
            "Second Citizen:\n",
            "Worthy Menenius Agrippa; one that hath always loved\n",
            "the people.\n",
            "\n",
            "First Citizen:\n",
            "He's one honest enough: would all the rest were so!\n",
            "\n",
            "MENENIUS:\n",
            "What work's, my countrymen, in hand? where go you\n",
            "With bats and clubs? The matter? speak, I pray you.\n",
            "\n",
            "First Citizen:\n",
            "Our business is not unknown to the senate; they have\n",
            "had inkling this fortnight what we intend to do,\n",
            "which now we'll show 'em in deeds. They say poor\n",
            "suitors have strong breaths: they shall know we\n",
            "have strong arms too.\n",
            "\n",
            "MENENIUS:\n",
            "Why, masters, my good friends, mine honest neighbours,\n",
            "Will you undo yourselves?\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## ¿Cómo son los datos?\n",
        "!cat input.txt | head -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSF4nLX9gmi3",
        "outputId": "2b72befb-f432-4e86-dcb3-0728f812fc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Vamos a entrenar carácteres en lugar de palabras\n",
        "!python data/shakespeare_char/prepare.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tNGNhLWgHqe",
        "outputId": "5edcd3c1-b74f-4c44-e334-dc9c6fe26e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: max_iters = 100\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: eval_interval = 50\n",
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: batch_size = 32\n",
            "tokens per iteration will be: 8,192\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2806\n",
            "[2023-05-15 18:07:09,355] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:10,163] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:11,938] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:12,524] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:12,942] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:13,267] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:13,683] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:14,000] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:14,406] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:14,715] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:15,128] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-15 18:07:15,431] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2673, time 27537.50ms, mfu -100.00%\n",
            "iter 10: loss 3.2243, time 61.36ms, mfu 3.04%\n",
            "iter 20: loss 2.8209, time 62.59ms, mfu 3.03%\n",
            "iter 30: loss 2.6313, time 62.28ms, mfu 3.03%\n",
            "iter 40: loss 2.5636, time 61.98ms, mfu 3.02%\n",
            "step 50: train loss 2.5241, val loss 2.5330\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "iter 50: loss 2.5404, time 4882.10ms, mfu 2.73%\n",
            "iter 60: loss 2.5175, time 62.29ms, mfu 2.75%\n",
            "iter 70: loss 2.5111, time 62.49ms, mfu 2.78%\n",
            "iter 80: loss 2.5290, time 62.66ms, mfu 2.80%\n",
            "iter 90: loss 2.5057, time 61.15ms, mfu 2.82%\n",
            "step 100: train loss 2.4717, val loss 2.5011\n",
            "saving checkpoint to out-shakespeare-char-100\n",
            "iter 100: loss 2.4899, time 846.71ms, mfu 2.56%\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Entrenamos un mini-GPT desde cero con 100 iteraciones\n",
        "!python train.py config/train_shakespeare_char.py --device='cuda' --max_iters=100 --eval_iters=20 --eval_interval=50 --out_dir='out-shakespeare-char-100' --batch_size=32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .codecarbon.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJGC6hGihJ1f",
        "outputId": "1c5581e0-fe8e-460d-88fe-1e68bc0f4ca4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[codecarbon]\n",
            "experiment_id = 0b6a2e04-5a19-4b52-a4c5-f89706997d57\n",
            "\n",
            "log_level = CRITICAL\n",
            "save_to_api = True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUvTvubzhIXZ",
        "outputId": "20909a31-c658-43e8-b039-8d8a1a864987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: device = cpu\n",
            "Overriding: out_dir = out-shakespeare-char-100\n",
            "Overriding: num_samples = 1\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "BR:\n",
            "Set tharsther he pis\n",
            "Cle haspthicer n me coury bes be wind s wincarerther nd w.\n",
            "Th\n",
            "IO;\n",
            "Go bowo allis bofofor'tho hers, wns hed puthart pe Ird t is:\n",
            "Thive theme don,\n",
            "Hotourd f\n",
            "Lurtheang f RORend tharty s the VULAy mou,\n",
            "\n",
            "ARI cr wis t atugil fo se xere fcorsif brme\n",
            "GORo youke mou'' ayonther ak\n",
            "S:\n",
            "Harm shan, t tha bas te tis the the athil thouspor athes the to t ceke pl tisharr t oom sen the pove g!\n",
            "S:\n",
            "amyom wer har:\n",
            "I tharefansenthe. b\n",
            "Ang wom.\n",
            "S:\n",
            "EODonthen ars t o'se se n cod bant wid ceapurdl\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Y el resultado es...\n",
        "!python sample.py --device='cpu' --out_dir='out-shakespeare-char-100' --num_samples=1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYirrvs6oEYh",
        "outputId": "b1eda060-934a-446d-de37-bbe7e7340167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 200\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: eval_interval = 100\n",
            "Overriding: out_dir = out-shakespeare-char-200\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2885, val loss 4.2831\n",
            "[2023-05-11 16:24:40,611] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:40,938] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:41,402] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:41,708] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:42,118] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:42,432] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:42,840] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:43,138] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:43,578] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:43,881] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:44,440] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "[2023-05-11 16:24:44,737] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2649, time 13739.72ms, mfu -100.00%\n",
            "iter 10: loss 3.2238, time 110.35ms, mfu 3.38%\n",
            "iter 20: loss 2.7689, time 111.88ms, mfu 3.37%\n",
            "iter 30: loss 2.6179, time 112.08ms, mfu 3.37%\n",
            "iter 40: loss 2.5692, time 110.69ms, mfu 3.37%\n",
            "iter 50: loss 2.5301, time 110.86ms, mfu 3.37%\n",
            "iter 60: loss 2.5140, time 109.61ms, mfu 3.37%\n",
            "iter 70: loss 2.4794, time 110.19ms, mfu 3.37%\n",
            "iter 80: loss 2.4902, time 109.95ms, mfu 3.37%\n",
            "iter 90: loss 2.4656, time 111.76ms, mfu 3.37%\n",
            "step 100: train loss 2.4577, val loss 2.4956\n",
            "saving checkpoint to out-shakespeare-char-200\n",
            "iter 100: loss 2.4657, time 3826.05ms, mfu 3.04%\n",
            "iter 110: loss 2.4483, time 111.52ms, mfu 3.07%\n",
            "iter 120: loss 2.4507, time 111.74ms, mfu 3.10%\n",
            "iter 130: loss 2.4376, time 111.38ms, mfu 3.12%\n",
            "iter 140: loss 2.4132, time 112.35ms, mfu 3.14%\n",
            "iter 150: loss 2.3855, time 114.60ms, mfu 3.15%\n",
            "iter 160: loss 2.3799, time 112.72ms, mfu 3.17%\n",
            "iter 170: loss 2.3390, time 113.85ms, mfu 3.18%\n",
            "iter 180: loss 2.3292, time 112.82ms, mfu 3.19%\n",
            "iter 190: loss 2.2425, time 112.58ms, mfu 3.20%\n",
            "step 200: train loss 2.1468, val loss 2.2135\n",
            "saving checkpoint to out-shakespeare-char-200\n",
            "iter 200: loss 2.2144, time 1450.15ms, mfu 2.91%\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Entrenamos un mini-GPT desde cero con 200 iteraciones\n",
        "!python train.py --device='cpu' config/train_shakespeare_char.py --max_iters=200 --eval_iters=20 --eval_interval=100 --out_dir='out-shakespeare-char-200' --batch_size=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWuao7NoeqX",
        "outputId": "84a46780-7801-4b82-cf73-625f2fb0b3a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: out_dir = out-shakespeare-char-200\n",
            "Overriding: num_samples = 1\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "BULES:\n",
            "BEST:\n",
            "Dord Exup theere, sptell with And to good be wild she thast ther now frod\n",
            "I gont buke all the forte't o hers,\n",
            "This tuld thart peairde wis: I tho the to mat rothe me beuet\n",
            "And, fat teld thatty ous havod bedo to speice an thing,\n",
            "Mine the xefin core f bringed mind ke mow's ay that?\n",
            "Thid thare shan, thand bes teare the t ndeat y necurs t ast!\n",
            "ORD:\n",
            "I g ticeke platio mar teno\n",
            "Isenck byoour hind da yom wee har:\n",
            "I t s thane nthe. bjeag wom.\n",
            "Sper Dent pe ars t o'se s ano wer ant wit co pubel\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Y el resultado con 200 iteraciones es...\n",
        "!python sample.py --out_dir='out-shakespeare-char-200' --num_samples=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGk2OOIJo3kv",
        "outputId": "79a43dfe-7942-4d79-b2a6-2b878358eacf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: device = cpu\n",
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: max_iters = 1000\n",
            "Overriding: log_interval = 100\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: eval_interval = 500\n",
            "Overriding: out_dir = out-shakespeare-char-1000\n",
            "Overriding: batch_size = 32\n",
            "tokens per iteration will be: 8,192\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: False\n",
            "compiling the model... (takes a ~minute)\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "step 0: train loss 4.2874, val loss 4.2806\n",
            "[2023-05-11 22:24:09,053] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
            "iter 0: loss 4.2671, time 279688.65ms, mfu -100.00%\n",
            "iter 100: loss 2.5056, time 17708.62ms, mfu 0.01%\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Entrenamos un mini-GPT desde cero con 1000 iteraciones\n",
        "!python train.py   --device='cpu' config/train_shakespeare_char.py --max_iters=1000 --log_interval=100 --eval_iters=20 --eval_interval=500 --out_dir='out-shakespeare-char-1000' --batch_size=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERJZxxYBp1HH",
        "outputId": "6d9b2b6d-71d7-4e6a-d7a7-02660d850d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: out_dir = out-shakespeare-char-1000\n",
            "Overriding: num_samples = 1\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "Become to brother Edward\n",
            "Than thou stealt recounty be for with men\n",
            "Bear the child from of Gloucesters.\n",
            "I do hither's heart, while that child;\n",
            "Inder it with the morrow portion me and torth,\n",
            "Which I bed, tyrantle Volsces,\n",
            "To should this way, the did axerited this brother mind.\n",
            "I more came to much a grace?\n",
            "\n",
            "PARIS:\n",
            "Go Talk to it the day at is the spire\n",
            "And steelign in your point.\n",
            "\n",
            "KING RICHARD II:\n",
            "A prince, my mouth mar: I was thank not,\n",
            "And against the day things store-sound to Rome it withouth and\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Y el resultado con 1000 iteraciones es mucho mejor... esto es aprendizaje a través de ejemplos\n",
        "\n",
        "!python sample.py --out_dir='out-shakespeare-char-1000' --num_samples=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUWhzAOhrta8",
        "outputId": "5f61c751-c5bc-4f0d-f26a-154e480a4cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/nanoGPT\n",
            "train has 301966 tokens\n",
            "val has 36059 tokens\n",
            "Overriding config with config/finetune_shakespeare.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-shakespeare'\n",
            "eval_interval = 5\n",
            "eval_iters = 40\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'shakespeare'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'shakespeare'\n",
            "init_from = 'gpt2' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 1\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 20\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "Overriding: max_iters = 40\n",
            "tokens per iteration will be: 32,768\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.1872, val loss 4.0326\n",
            "iter 0: loss 4.6559, time 32603.04ms, mfu -100.00%\n",
            "iter 1: loss 3.6389, time 3072.92ms, mfu -100.00%\n",
            "iter 2: loss 3.7510, time 3105.31ms, mfu -100.00%\n",
            "iter 3: loss 3.8442, time 3123.05ms, mfu -100.00%\n",
            "iter 4: loss 3.7711, time 3131.24ms, mfu -100.00%\n",
            "step 5: train loss 3.6273, val loss 3.4013\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 5: loss 4.0611, time 11150.23ms, mfu 0.81%\n",
            "iter 6: loss 3.8367, time 3050.82ms, mfu 1.02%\n",
            "iter 7: loss 3.8424, time 3044.59ms, mfu 1.21%\n",
            "iter 8: loss 4.2738, time 3045.25ms, mfu 1.39%\n",
            "iter 9: loss 3.6958, time 3045.84ms, mfu 1.54%\n",
            "step 10: train loss 3.6092, val loss 3.3710\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 10: loss 3.3603, time 11032.22ms, mfu 1.47%\n",
            "iter 11: loss 3.8340, time 2995.07ms, mfu 1.62%\n",
            "iter 12: loss 3.4308, time 2998.69ms, mfu 1.76%\n",
            "iter 13: loss 3.5108, time 3006.91ms, mfu 1.88%\n",
            "iter 14: loss 3.4060, time 3006.11ms, mfu 1.99%\n",
            "step 15: train loss 3.4972, val loss 3.3997\n",
            "iter 15: loss 3.1891, time 4912.78ms, mfu 1.98%\n",
            "iter 16: loss 2.6357, time 3026.16ms, mfu 2.08%\n",
            "iter 17: loss 3.8017, time 3036.45ms, mfu 2.16%\n",
            "iter 18: loss 3.4124, time 3031.74ms, mfu 2.24%\n",
            "iter 19: loss 3.2706, time 3043.27ms, mfu 2.31%\n",
            "step 20: train loss 3.4279, val loss 3.4041\n",
            "iter 20: loss 3.4981, time 4943.63ms, mfu 2.26%\n",
            "iter 21: loss 3.4422, time 3040.09ms, mfu 2.33%\n",
            "iter 22: loss 3.9077, time 3044.08ms, mfu 2.40%\n",
            "iter 23: loss 3.5729, time 3039.00ms, mfu 2.45%\n",
            "iter 24: loss 4.0962, time 3042.89ms, mfu 2.50%\n",
            "step 25: train loss 3.4131, val loss 3.2819\n",
            "saving checkpoint to out-shakespeare\n",
            "iter 25: loss 3.2789, time 10957.49ms, mfu 2.33%\n",
            "iter 26: loss 3.7309, time 3011.89ms, mfu 2.40%\n",
            "iter 27: loss 3.7979, time 3020.46ms, mfu 2.46%\n",
            "iter 28: loss 3.1843, time 3023.05ms, mfu 2.51%\n",
            "iter 29: loss 3.4577, time 3024.04ms, mfu 2.55%\n",
            "step 30: train loss 3.3483, val loss 3.3522\n",
            "iter 30: loss 3.0863, time 4966.95ms, mfu 2.48%\n",
            "iter 31: loss 3.2291, time 3032.20ms, mfu 2.53%\n",
            "iter 32: loss 3.4391, time 3038.36ms, mfu 2.57%\n",
            "iter 33: loss 3.1196, time 3035.73ms, mfu 2.61%\n",
            "iter 34: loss 3.2774, time 3043.46ms, mfu 2.64%\n",
            "step 35: train loss 3.3409, val loss 3.2984\n",
            "iter 35: loss 3.7264, time 4983.89ms, mfu 2.56%\n",
            "iter 36: loss 3.4445, time 3035.62ms, mfu 2.60%\n",
            "iter 37: loss 3.3505, time 3034.97ms, mfu 2.64%\n",
            "iter 38: loss 3.6827, time 3028.54ms, mfu 2.67%\n",
            "iter 39: loss 3.4549, time 3041.88ms, mfu 2.70%\n",
            "step 40: train loss 3.3789, val loss 3.2992\n",
            "iter 40: loss 3.2666, time 4952.01ms, mfu 2.61%\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Finetune shakespeare con palabras (en vez de carácteres)\n",
        "%cd /content/nanoGPT/\n",
        "from data.shakespeare.prepare import Dataset\n",
        "import logging, os\n",
        "\n",
        "# Set logging to debug\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "# Example using shakespeare\n",
        "ds = Dataset()\n",
        "ds.fetch()\n",
        "ds.save('input.txt')\n",
        "ds.load('input.txt')\n",
        "ds.parse()\n",
        "ds.export('./data/shakespeare/')\n",
        "!python train.py config/finetune_shakespeare.py --max_iters=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGJU3yA5gFb3",
        "outputId": "8203d5ba-1058-4cbb-a18e-ec9100fbf51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: out_dir = out-shakespeare\n",
            "Overriding: num_samples = 1\n",
            "Overriding: device = cuda\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "\n",
            "- * * All creatures that are created in the image of God are created in the image of man.\n",
            "\n",
            "NASB.: And now again the LORD spoke to the sons of men; but he spoke not.\n",
            "\n",
            "LICENSUS: Then he did command them,\n",
            "As God himself commanded, that they should speak unto man,\n",
            "So far as they came: and they came unto him,\n",
            "And he commanded them,\n",
            "And the LORD did command them,\n",
            "\n",
            "Say, we are given up unto thee: say, I have seen thee;\n",
            "I have seen thee in the shape of an eagle\n",
            "And in the presence of a dove: and thou art mineself.\n",
            "\n",
            "JESUS: So I say unto thee, I am a dove.\n",
            "\n",
            "LICENSUS: Now, then, let us say to them,\n",
            "That thou art mineself; thou art mineself,\n",
            "I am a dove; I am a dove.\n",
            "\n",
            "JESUS: But thou art mineself,\n",
            "I am a dove; thou art mineself,\n",
            "I am a dove.\n",
            "\n",
            "LICENSUS: And, if thou be mineself; thou art mineself,\n",
            "I am a dove.\n",
            "\n",
            "JESUS: But thou art mineself,\n",
            "I am a dove; thou art mineself,\n",
            "I am a dove.\n",
            "\n",
            "LICENSUS: Behold, let us say\n",
            "To thee, of the human tongue;\n",
            "Dost thou say, and I fear thee?\n",
            "\n",
            "JESUS: Yes, I say, I fear thee.\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "JESUS:\n",
            "\n",
            "LICENSUS:\n",
            "\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Resultado con 40 iteraciones adaptando un GPT2 pre-entrenado\n",
        "!python sample.py --out_dir=out-shakespeare --num_samples=1 --device='cuda' --max_new_tokens=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6q5QE5FygnM",
        "outputId": "9562047e-23af-4340-9f28-35b48e17ac86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding: out_dir = out-shakespeare\n",
            "Overriding: start = KING RICHARD II:          What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 2\n",
            "Overriding: max_new_tokens = 100\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "KING RICHARD II:          What is the answer to life, the universe, and everything? A:             \n",
            "\n",
            "THE ORATORY ORATES OF THE KING WOLF:\n",
            "\n",
            "The other council of Christ,\n",
            "Which was gathered to this council,\n",
            "And there as a council,\n",
            "And not to a council as an assembly;\n",
            "So that the king, who was the king's council,\n",
            "Would stand before the king's council;\n",
            "And as a council it was,\n",
            "That he should stand before his\n",
            "---------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/sample.py\", line 87, in <module>\n",
            "    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 323, in generate\n",
            "    logits, _ = self(idx_cond)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 188, in forward\n",
            "    x = block(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 112, in forward\n",
            "    x = x + self.mlp(self.ln_2(x))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 97, in forward\n",
            "    x = self.c_proj(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Utilizando la técnica de \"Prompting\" para preguntar y sesgar la salida del modelo\n",
        "!python sample.py \\\n",
        "    --out_dir=out-shakespeare \\\n",
        "    --start=\"KING RICHARD II: \\\n",
        "        What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=2 --max_new_tokens=100 --device=cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN5O3AUx0bhR",
        "outputId": "c6efebb4-e2c5-47a5-af01-17436bf2e800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 799499 tokens\n",
            "val has 86982 tokens\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Con castellano?? Probamos con Calderón de la Barca. Los sueños, sueños son... \n",
        "from data.calderon.prepare import Dataset\n",
        "import logging, os\n",
        "\n",
        "# Set logging to debug\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "# Example using shakespeare\n",
        "ds = Dataset()\n",
        "ds.load('./data/calderon/cap1-4.txt')\n",
        "ds.parse()\n",
        "ds.export('./data/calderon/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LCbjOf0gp1",
        "outputId": "0d49ae86-01cb-4d75-a892-088a64c435f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿ESCENA PRIMERA.\r\n",
            "\r\n",
            "ROSAURA, CLARIN.\r\n",
            "\r\n",
            "_(Rosaura vestida de hombre aparece en lo alto de las peñas, y baja á\r\n",
            "lo llano; tras ella viene Clarin.)_\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  Hipogrifo violento\r\n",
            "  Que corriste parejas con el viento,\r\n",
            "  ¿Dónde rayo sin llama,\r\n",
            "  Pájaro sin matiz, pez sin escama,\r\n",
            "  Y bruto sin instinto\r\n",
            "  Natural, al confuso laberinto\r\n",
            "  Destas desnudas peñas\r\n",
            "  Te desbocas, arrastras y despeñas?\r\n",
            "  Quédate en este monte,\r\n",
            "  Donde tengan los brutos su Faetonte;\r\n",
            "  Que yo, sin más camino\r\n",
            "  Que el que me dan las leyes del destino.\r\n",
            "  Ciega y desesperada\r\n",
            "  Bajaré la aspereza enmarañada\r\n",
            "  Deste monte eminente,\r\n",
            "  Que arruga al sol el ceño de su frente.\r\n",
            "  Mal, Polonia, recibes\r\n",
            "  A un extranjero, pues con sangre escribes\r\n",
            "  Su entrada en tus arenas,\r\n",
            "  Y apénas llega, cuando llega á penas.\r\n",
            "  Bien mi suerte lo dice;\r\n",
            "  ¿Mas dónde halló piedad un infelice?\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "  Dí dos, y no me dejes\r\n",
            "  En la posada á mí cuando te quejes;\r\n",
            "  Que si dos hemos sido\r\n",
            "  Los que de nuestra patria hemos salido\r\n",
            "  A probar aventuras,\r\n",
            "  Dos los que entre desdichas y locuras\r\n",
            "  Aquí habemos llegado,\r\n",
            "  Y dos los que del monte hemos rodado,\r\n",
            "  ¿No es razon que yo sienta\r\n",
            "  Meterme en el pesar, y no en la cuenta?\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  No te quiero dar parte\r\n",
            "  En mis quejas, Clarin, por no quitarte,\r\n",
            "  Llorando tu desvelo,\r\n",
            "  El derecho que tienes tú al consuelo.\r\n",
            "  Que tanto gusto habia\r\n",
            "  En quejarse, un filósofo decia,\r\n",
            "  Que, á trueco de quejarse,\r\n",
            "  Habian las desdichas de buscarse.\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "  El filósofo era\r\n",
            "  Un borracho barbon: ¡oh! ¡quién le diera\r\n",
            "  Más de mil bofetadas!\r\n",
            "  Quejárase despues de muy bien dadas.\r\n",
            "  ¿Mas qué haremos, señora,\r\n",
            "  A pié, solos, perdidos y á esta hora\r\n",
            "  En un desierto monte,\r\n",
            "  Cuando se parte el sol á otro horizonte?\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  ¡Quién ha visto sucesos tan extraños!\r\n",
            "  Mas si la vista no padece engaños\r\n",
            "  Que hace la fantasía,\r\n",
            "  A la medrosa luz que áun tiene el dia,\r\n",
            "  Me parece que veo\r\n",
            "  Un edificio.\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "               Ó miente mi deseo,\r\n",
            "  Ó termino las señas.\r\n",
            "\r\n",
            "ROSAURA.\r\n",
            "\r\n",
            "  Rústico nace entre desnudas peñas\r\n",
            "  Un palacio tan breve,\r\n",
            "  Que al sol apénas á mirar se atreve:\r\n",
            "  Con tan rudo artificio\r\n",
            "  La arquitectura está de su edificio,\r\n",
            "  Que parece, á las plantas\r\n",
            "  De tantas rocas y de peñas tantas\r\n",
            "  Que al sol tocan la lumbre,\r\n",
            "  Peñasco que ha rodado de la cumbre.\r\n",
            "\r\n",
            "CLARIN.\r\n",
            "\r\n",
            "  Vámonos acercando;\r\n",
            "  Que este es mucho mirar, señora, cuando\r\n",
            "  Es mejor que la gente\r\n",
            "  Que habita en ella, generosamente\r\n",
            "  Nos admita.\r\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## ¿Cómo son los datos?\n",
        "!cat ./data/calderon/cap1-4.txt | head -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoUdHnXb14W8",
        "outputId": "fd9ec27e-6040-4342-daf6-fccb74c3443a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/finetune_calderon.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-calderon'\n",
            "eval_interval = 100\n",
            "eval_iters = 500\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'calderon'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'calderon'\n",
            "init_from = 'gpt2' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 2\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 200\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "Overriding: init_from = gpt2-medium\n",
            "Overriding: max_iters = 40\n",
            "Overriding: eval_interval = 20\n",
            "Overriding: eval_iters = 40\n",
            "Overriding: device = cuda\n",
            "tokens per iteration will be: 65,536\n",
            "Initializing from OpenAI GPT-2 weights: gpt2-medium\n",
            "loading weights from pretrained gpt: gpt2-medium\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 353.77M\n",
            "num decayed parameter tensors: 98, with 354,501,632 parameters\n",
            "num non-decayed parameter tensors: 194, with 321,536 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 3.4004, val loss 3.4493\n",
            "iter 0: loss 3.1954, time 71965.94ms, mfu -100.00%\n",
            "iter 1: loss 3.1372, time 11675.25ms, mfu -100.00%\n",
            "iter 2: loss 2.7209, time 12084.45ms, mfu -100.00%\n",
            "iter 3: loss 3.2828, time 12674.44ms, mfu -100.00%\n",
            "iter 4: loss 3.2008, time 12520.40ms, mfu -100.00%\n",
            "iter 5: loss 3.2631, time 12192.33ms, mfu 4.18%\n",
            "iter 6: loss 3.1379, time 12037.77ms, mfu 4.18%\n",
            "iter 7: loss 2.9381, time 11936.72ms, mfu 4.19%\n",
            "iter 8: loss 2.7013, time 12016.76ms, mfu 4.20%\n",
            "iter 9: loss 2.8017, time 12178.68ms, mfu 4.19%\n",
            "iter 10: loss 2.8341, time 12290.95ms, mfu 4.19%\n",
            "iter 11: loss 2.9482, time 12305.11ms, mfu 4.18%\n",
            "iter 12: loss 2.6101, time 12244.22ms, mfu 4.18%\n",
            "iter 13: loss 2.8411, time 12172.80ms, mfu 4.18%\n",
            "iter 14: loss 2.6184, time 12130.26ms, mfu 4.18%\n",
            "iter 15: loss 2.6301, time 12153.70ms, mfu 4.18%\n",
            "iter 16: loss 2.2528, time 12189.17ms, mfu 4.18%\n",
            "iter 17: loss 2.7271, time 12185.86ms, mfu 4.18%\n",
            "iter 18: loss 2.6492, time 12229.02ms, mfu 4.18%\n",
            "iter 19: loss 2.3212, time 12221.20ms, mfu 4.18%\n",
            "step 20: train loss 2.7097, val loss 2.8944\n",
            "saving checkpoint to out-calderon\n",
            "iter 20: loss 2.9468, time 73817.92ms, mfu 3.83%\n",
            "iter 21: loss 2.7379, time 12650.81ms, mfu 3.85%\n",
            "iter 22: loss 2.9908, time 12679.61ms, mfu 3.87%\n",
            "iter 23: loss 2.7555, time 12251.19ms, mfu 3.90%\n",
            "iter 24: loss 2.3592, time 12004.04ms, mfu 3.93%\n",
            "iter 25: loss 2.2994, time 11945.01ms, mfu 3.96%\n",
            "iter 26: loss 3.0039, time 12025.55ms, mfu 3.99%\n",
            "iter 27: loss 2.8570, time 12228.56ms, mfu 4.01%\n",
            "iter 28: loss 2.4135, time 12348.95ms, mfu 4.02%\n",
            "iter 29: loss 2.0611, time 12310.28ms, mfu 4.03%\n",
            "iter 30: loss 3.0191, time 12210.39ms, mfu 4.05%\n",
            "iter 31: loss 2.3413, time 12101.36ms, mfu 4.06%\n",
            "iter 32: loss 2.6507, time 12133.32ms, mfu 4.08%\n",
            "iter 33: loss 2.3478, time 12169.11ms, mfu 4.09%\n",
            "iter 34: loss 2.3565, time 12203.03ms, mfu 4.10%\n",
            "iter 35: loss 2.3663, time 12247.63ms, mfu 4.10%\n",
            "iter 36: loss 2.2725, time 12201.96ms, mfu 4.11%\n",
            "iter 37: loss 2.6469, time 12176.65ms, mfu 4.12%\n",
            "iter 38: loss 2.3986, time 12122.51ms, mfu 4.12%\n",
            "iter 39: loss 2.1167, time 12136.39ms, mfu 4.13%\n",
            "step 40: train loss 2.4525, val loss 2.7481\n",
            "saving checkpoint to out-calderon\n",
            "iter 40: loss 2.3110, time 56896.75ms, mfu 3.81%\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Resultado con 40tf iteraciones adaptando un GPT2 pre-entrenado\n",
        "!rm -rf ./out-calderon\n",
        "!python train.py config/finetune_calderon.py --init_from='gpt2-medium' --max_iters=40 --eval_interval=20 --eval_iters=40 --device='cuda'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Resultado con 40 iteraciones adaptando un GPT2 pre-entrenado\n",
        "!python sample.py --out_dir=out-calderon --num_samples=1 --device='cpu'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRZz9Yn4psWy",
        "outputId": "5346ba07-7c7b-42ec-8971-63da78e75d1a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-calderon\n",
            "Overriding: num_samples = 1\n",
            "Overriding: device = cpu\n",
            "number of parameters: 353.77M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/sample.py\", line 87, in <module>\n",
            "    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 325, in generate\n",
            "    logits, _ = self(idx_cond)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 190, in forward\n",
            "    x = block(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 113, in forward\n",
            "    x = x + self.mlp(self.ln_2(x))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 98, in forward\n",
            "    x = self.c_proj(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Entrenamos con 40 iteraciones más\n",
        "!python train.py config/finetune_calderon.py --init_from='resume' --device='cuda' --max_iters=80 --eval_interval=20 --eval_iters=40 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0btvGuMpR3E",
        "outputId": "02af5e1f-a1ef-41d8-b98a-27e9e3dc38f1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/finetune_calderon.py:\n",
            "import time\n",
            "\n",
            "out_dir = 'out-calderon'\n",
            "eval_interval = 100\n",
            "eval_iters = 500\n",
            "wandb_log = False # feel free to turn on\n",
            "wandb_project = 'calderon'\n",
            "wandb_run_name = 'ft-' + str(time.time())\n",
            "\n",
            "dataset = 'calderon'\n",
            "init_from = 'gpt2' # this is the largest GPT-2 model\n",
            "\n",
            "# only save checkpoints if the validation loss improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "# the number of examples per iter:\n",
            "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
            "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
            "batch_size = 2\n",
            "gradient_accumulation_steps = 32\n",
            "max_iters = 200\n",
            "\n",
            "# finetune at constant LR\n",
            "learning_rate = 3e-5\n",
            "decay_lr = False\n",
            "\n",
            "Overriding: init_from = resume\n",
            "Overriding: device = cuda\n",
            "Overriding: max_iters = 80\n",
            "Overriding: eval_interval = 15\n",
            "Overriding: eval_iters = 40\n",
            "tokens per iteration will be: 65,536\n",
            "Resuming training from out-calderon\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "iter 40: loss 2.9698, time 23691.16ms, mfu -100.00%\n",
            "iter 41: loss 2.9836, time 4672.52ms, mfu -100.00%\n",
            "iter 42: loss 2.5032, time 4723.73ms, mfu -100.00%\n",
            "iter 43: loss 2.8841, time 4744.52ms, mfu -100.00%\n",
            "iter 44: loss 2.8165, time 4783.10ms, mfu -100.00%\n",
            "step 45: train loss 2.7871, val loss 3.0284\n",
            "iter 45: loss 2.7867, time 15018.22ms, mfu 1.20%\n",
            "iter 46: loss 2.7666, time 4899.83ms, mfu 1.44%\n",
            "iter 47: loss 2.9636, time 4917.20ms, mfu 1.66%\n",
            "iter 48: loss 2.4383, time 4966.09ms, mfu 1.86%\n",
            "iter 49: loss 2.9416, time 5003.99ms, mfu 2.03%\n",
            "iter 50: loss 2.6646, time 5050.95ms, mfu 2.18%\n",
            "iter 51: loss 2.6851, time 5088.73ms, mfu 2.32%\n",
            "iter 52: loss 2.7757, time 5102.49ms, mfu 2.44%\n",
            "iter 53: loss 2.4047, time 5105.02ms, mfu 2.55%\n",
            "iter 54: loss 2.5475, time 5055.85ms, mfu 2.65%\n",
            "iter 55: loss 2.3394, time 5009.28ms, mfu 2.74%\n",
            "iter 56: loss 3.0972, time 4998.03ms, mfu 2.83%\n",
            "iter 57: loss 3.0584, time 4979.48ms, mfu 2.90%\n",
            "iter 58: loss 2.7767, time 4965.20ms, mfu 2.98%\n",
            "iter 59: loss 2.2599, time 4956.78ms, mfu 3.04%\n",
            "step 60: train loss 2.6695, val loss 2.9345\n",
            "saving checkpoint to out-calderon\n",
            "iter 60: loss 2.8686, time 20933.80ms, mfu 2.82%\n",
            "iter 61: loss 3.0392, time 4910.54ms, mfu 2.91%\n",
            "iter 62: loss 2.2130, time 4986.08ms, mfu 2.98%\n",
            "iter 63: loss 2.7086, time 5024.26ms, mfu 3.04%\n",
            "iter 64: loss 2.9450, time 5070.53ms, mfu 3.09%\n",
            "iter 65: loss 2.5738, time 5127.27ms, mfu 3.13%\n",
            "iter 66: loss 2.6917, time 5150.53ms, mfu 3.16%\n",
            "iter 67: loss 2.4737, time 5136.11ms, mfu 3.20%\n",
            "iter 68: loss 2.6395, time 5089.46ms, mfu 3.23%\n",
            "iter 69: loss 2.5440, time 5050.08ms, mfu 3.26%\n",
            "iter 70: loss 2.7944, time 5003.09ms, mfu 3.30%\n",
            "iter 71: loss 2.7955, time 4989.14ms, mfu 3.33%\n",
            "iter 72: loss 2.3697, time 4971.81ms, mfu 3.36%\n",
            "iter 73: loss 2.8788, time 4957.11ms, mfu 3.38%\n",
            "iter 74: loss 2.5847, time 4956.78ms, mfu 3.41%\n",
            "step 75: train loss 2.6470, val loss 2.9234\n",
            "saving checkpoint to out-calderon\n",
            "iter 75: loss 2.8334, time 15133.34ms, mfu 3.18%\n",
            "iter 76: loss 2.4845, time 4943.12ms, mfu 3.23%\n",
            "iter 77: loss 2.1010, time 4991.29ms, mfu 3.27%\n",
            "iter 78: loss 2.5591, time 5032.70ms, mfu 3.30%\n",
            "iter 79: loss 2.5356, time 5067.16ms, mfu 3.32%\n",
            "iter 80: loss 2.6162, time 5108.13ms, mfu 3.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHzsc0Ix2M2u",
        "outputId": "9a88fcfc-36e7-4e4e-9367-13c985024868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-calderon\n",
            "Overriding: num_samples = 1\n",
            "Overriding: device = cpu\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPT/sample.py\", line 87, in <module>\n",
            "    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 325, in generate\n",
            "    logits, _ = self(idx_cond)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 190, in forward\n",
            "    x = block(x)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 112, in forward\n",
            "    x = x + self.attn(self.ln_1(x))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/nanoGPT/model.py\", line 65, in forward\n",
            "    q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@markdown ## Resultado con 80 iteraciones adaptando un GPT2 pre-entrenado\n",
        "!python sample.py --out_dir=out-calderon --num_samples=1 --device='cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSppj6Pi2O4m"
      },
      "outputs": [],
      "source": [
        "#@markdown ## y ya me he cansado por hoy..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}